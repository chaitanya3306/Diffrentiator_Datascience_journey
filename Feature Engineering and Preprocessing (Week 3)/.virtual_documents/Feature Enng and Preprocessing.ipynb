import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns





filepath='Data/bank_loan_dataset.csv'
df=pd.read_csv(filepath)


print("Shape:")
df.shape



print('first five lines :')


print('Columns:')
df.columns.to_list()


print('basic info :')
df.info()



# missing values:
df.isnull().sum()





# dealing with the null values:
df['Gender']=df['Gender'].fillna('unknown')
mode_value=df['Married'].mode()[0]
df['Married']=df['Married'].fillna(mode_value)
df['LoanAmount']=df.groupby(['Education','Self_Employed'])['LoanAmount'].transform(lambda x:x.fillna(x.median()))
df['Loan_Amount_Term']=df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mean())
df['Credit_History']=df['Credit_History'].fillna(df['Credit_History'].mean())
df.isnull().sum()


# duplicate values:
df.duplicated().sum()





# describing the datasest in the terms :
df.describe(include=['int64','float64']).T





df.head()


# identify the unique values of the dataset:

for col in df.columns:
    unique_vals=df[col].unique()
    print(f"{col} : {unique_vals}  ")


#  Check data types & distributions
# Purpose: Detect wrong types (e.g., numbers stored as strings, dates as objects)


df.dtypes


# there is no date variable if its there we have to convert it to the datetime fuctions
# convert categorical variavles dtype into category:

categorical_vars=df.select_dtypes(include=['object'])
for cols in categorical_vars:
    df[cols]=df[cols].astype('category')

df.dtypes


# Detect string inconsistencies
cate_cols=df.select_dtypes(include='category').columns
for col in cate_cols:
    df[col]=df[col].str.strip().str.title()


df.head()


# Check numeric feature distributions
# Purpose: Identify skewness, scaling needs, and outliers

num_cols=df.select_dtypes(include=['int64','float64']).columns
for col in num_cols:
    plt.figure(figsize=(6,3))
    sns.histplot(df[col],kde=True)
    plt.title(f'Distribution of {col}')
    plt.show()






df.select_dtypes(include='object').columns.tolist()











# OHE for property_area:
encoded_df=pd.get_dummies(df,columns=['Property_Area'],drop_first=True)


# OHE on gender:
encoded_df=pd.get_dummies(df,columns=['Gender'],prefix='Gender',dummy_na=False,drop_first=True)








# we use mapping for this :

edu_map={'Not Graduate':0,'Graduate':1}
encoded_df['Education_Encoded']=df['Education'].map(edu_map)

encoded_df[['Education','Education_Encoded']]








# we have to generate the map for the implementing the freq encoding :
freq_map=encoded_df['Property_Area'].value_counts().to_dict()
encoded_df['Property_Area_freq']=encoded_df['Property_Area'].map(freq_map)
encoded_df[['Property_Area','Property_Area_freq']].head()








# Load_status are binary:

encoded_df['Loan_Status']=encoded_df['Loan_Status'].map({'Y':1,'N':0})
encoded_df['Married']=encoded_df['Married'].map({'Yes':1,'No':0})
encoded_df['Self_Employed']=encoded_df['Self_Employed'].map({'Yes':1,'No':0})


encoded_df














from sklearn.model_selection import train_test_split


train_df,test_df=train_test_split(encoded_df,test_size=0.2,random_state=42)

# target_means=train_df.groupby('Property_Area')['Loan_Status'].mean()
target_means =train_df.groupby('Property_Area')['Loan_Status'].mean()

train_df['Property_Area_TE']=train_df['Property_Area'].map(target_means)
test_df['Property_Area_TE']=test_df['Property_Area'].map(target_means)

train_df[['Property_Area','Property_Area_TE']].head()


train_df,test_df=train_test_split(encoded_df,test_size=0.2,random_state=42)

target_means=train_df.groupby('Married')['Loan_Status'].mean()
train_df['Married_TE']=train_df['Married'].map(target_means)
test_df['Married_TE']=test_df['Married'].map(target_means)

train_df[['Married','Married_TE']]














# we have to apply the scaling and normalizing methods on the numerical type variables:
df.select_dtypes(include=['int64','float64']).columns.tolist()
df.dtypes





encoded_df


from sklearn.preprocessing import StandardScaler


scaler=StandardScaler()
encoded_df[['ApplicantIncome_scaled','LoanAmount_scaled']]=scaler.fit_transform(encoded_df[['ApplicantIncome','LoanAmount']])
encoded_df[['ApplicantIncome_scaled','LoanAmount_scaled']].head()




















from sklearn.preprocessing import MinMaxScaler


scaler=MinMaxScaler()
encoded_df[["ApplicantIncome_norm","LoanAmount_norm"]]=scaler.fit_transform(encoded_df[["ApplicantIncome","LoanAmount"]])
encoded_df[['ApplicantIncome_norm','LoanAmount_norm']].head()


from sklearn.preprocessing import RobustScaler


scaler=RobustScaler()
encoded_df[['CoapplicantIncome_norm']]=scaler.fit_transform(encoded_df[['CoapplicantIncome']])
encoded_df[['CoapplicantIncome_norm']]



