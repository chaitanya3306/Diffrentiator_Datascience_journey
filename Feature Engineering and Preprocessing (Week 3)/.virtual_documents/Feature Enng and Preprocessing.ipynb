import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns





filepath='Data/bank_loan_dataset.csv'
df=pd.read_csv(filepath)


print("Shape:")
df.shape



print('first five lines :')


print('Columns:')
df.columns.to_list()


print('basic info :')
df.info()



# missing values:
df.isnull().sum()





# dealing with the null values:
df['Gender']=df['Gender'].fillna('unknown')
mode_value=df['Married'].mode()[0]
df['Married']=df['Married'].fillna(mode_value)
df['LoanAmount']=df.groupby(['Education','Self_Employed'])['LoanAmount'].transform(lambda x:x.fillna(x.median()))
df['Loan_Amount_Term']=df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mean())
df['Credit_History']=df['Credit_History'].fillna(df['Credit_History'].mean())
df.isnull().sum()


# duplicate values:
df.duplicated().sum()





# describing the datasest in the terms :
df.describe(include=['int64','float64']).T





df.head()


# identify the unique values of the dataset:

for col in df.columns:
    unique_vals=df[col].unique()
    print(f"{col} : {unique_vals}  ")


#  Check data types & distributions
# Purpose: Detect wrong types (e.g., numbers stored as strings, dates as objects)


df.dtypes


# there is no date variable if its there we have to convert it to the datetime fuctions
# convert categorical variavles dtype into category:

categorical_vars=df.select_dtypes(include=['object'])
for cols in categorical_vars:
    df[cols]=df[cols].astype('category')

df.dtypes


# Detect string inconsistencies
cate_cols=df.select_dtypes(include='category').columns
for col in cate_cols:
    df[col]=df[col].str.strip().str.title()


df.head()


# Check numeric feature distributions
# Purpose: Identify skewness, scaling needs, and outliers

num_cols=df.select_dtypes(include=['int64','float64']).columns
for col in num_cols:
    plt.figure(figsize=(6,3))
    sns.histplot(df[col],kde=True)
    plt.title(f'Distribution of {col}')
    plt.show()






df.select_dtypes(include='object').columns.tolist()











# OHE for property_area:
encoded_df=pd.get_dummies(df,columns=['Property_Area'],drop_first=True)


# OHE on gender:
encoded_df=pd.get_dummies(df,columns=['Gender'],prefix='Gender',dummy_na=False,drop_first=True)








# we use mapping for this :

edu_map={'Not Graduate':0,'Graduate':1}
encoded_df['Education_Encoded']=df['Education'].map(edu_map)

encoded_df[['Education','Education_Encoded']]








# we have to generate the map for the implementing the freq encoding :
freq_map=encoded_df['Property_Area'].value_counts().to_dict()
encoded_df['Property_Area_freq']=encoded_df['Property_Area'].map(freq_map)
encoded_df[['Property_Area','Property_Area_freq']].head()








# Load_status are binary:

encoded_df['Loan_Status']=encoded_df['Loan_Status'].map({'Y':1,'N':0})
encoded_df['Married']=encoded_df['Married'].map({'Yes':1,'No':0})
encoded_df['Self_Employed']=encoded_df['Self_Employed'].map({'Yes':1,'No':0})


encoded_df














from sklearn.model_selection import train_test_split


train_df,test_df=train_test_split(encoded_df,test_size=0.2,random_state=42)

# target_means=train_df.groupby('Property_Area')['Loan_Status'].mean()
target_means =train_df.groupby('Property_Area')['Loan_Status'].mean()

train_df['Property_Area_TE']=train_df['Property_Area'].map(target_means)
test_df['Property_Area_TE']=test_df['Property_Area'].map(target_means)

train_df[['Property_Area','Property_Area_TE']].head()


train_df,test_df=train_test_split(encoded_df,test_size=0.2,random_state=42)

target_means=train_df.groupby('Married')['Loan_Status'].mean()
train_df['Married_TE']=train_df['Married'].map(target_means)
test_df['Married_TE']=test_df['Married'].map(target_means)

train_df[['Married','Married_TE']]














# we have to apply the scaling and normalizing methods on the numerical type variables:
df.select_dtypes(include=['int64','float64']).columns.tolist()
df.dtypes





encoded_df


from sklearn.preprocessing import StandardScaler


scaler=StandardScaler()
encoded_df[['ApplicantIncome_scaled','LoanAmount_scaled']]=scaler.fit_transform(encoded_df[['ApplicantIncome','LoanAmount']])
encoded_df[['ApplicantIncome_scaled','LoanAmount_scaled']].head()




















from sklearn.preprocessing import MinMaxScaler


scaler=MinMaxScaler()
encoded_df[["ApplicantIncome_norm","LoanAmount_norm"]]=scaler.fit_transform(encoded_df[["ApplicantIncome","LoanAmount"]])
encoded_df[['ApplicantIncome_norm','LoanAmount_norm']].head()





from sklearn.preprocessing import RobustScaler





scaler=RobustScaler()
encoded_df[['CoapplicantIncome_norm']]=scaler.fit_transform(encoded_df[['CoapplicantIncome']])
encoded_df[['CoapplicantIncome_norm']]



plt.figure(figsize=(4,2))
sns.histplot(data=encoded_df['CoapplicantIncome'])
plt.title('Coapplicant_Income')
plt.show()


plt.figure(figsize=(4,2))
sns.histplot(data=encoded_df['CoapplicantIncome_norm'],bins=4)
plt.title('Coapplicant_Income_norm')
plt.show()








from sklearn.preprocessing import MaxAbsScaler


scaler=MaxAbsScaler()
encoded_df[['ApplicantIncome_scaled']]=scaler.fit_transform(encoded_df[['ApplicantIncome']])
encoded_df[['ApplicantIncome_scaled']]


encoded_df.head()





df.head()


#lets check the numerical ones:
df.select_dtypes(include=['int64','float64'])


skew_df=df
# we have to plot the histplot for comparing the transformation so lets make function for it:
def histplot(col,title):
    sns.histplot(df[col],kde=True)
    plt.title(title)
    plt.figtext(0.5,0,f'Skewness :{skew_df[col].skew()}', horizontalalignment='center') #
    plt.show()
    

#lets checks skewness of applicantincome column :
print('Skewness ::',skew_df['ApplicantIncome'].skew())
histplot('ApplicantIncome','Before')


#lets take the appliacntincome column 

# so it has many large values we have to use log or sqrt or we can also use boxcox method 

skew_df['ApplicantIncome_log']=np.log1p(df['ApplicantIncome'])
skew_df[['ApplicantIncome','ApplicantIncome_log']]


histplot('ApplicantIncome_log','After')


#now lets skewness of the coapplicanincome column:
print(f'Skewness  : {skew_df['CoapplicantIncome'].skew()}')
histplot('CoapplicantIncome','Before')





skew_df['CoapplicantIncome_log']=np.log1p(skew_df['CoapplicantIncome'])


histplot('CoapplicantIncome_log','After')






#now apply the BoxCox on the loan amount
from scipy.stats import boxcox

transformed_data,best_lambda=boxcox(skew_df['LoanAmount'])

print(f"optimal lambda: {best_lambda}")









from sklearn.preprocessing import PolynomialFeatures


encoded_df


df.drop(['ApplicantIncome_log','CoapplicantIncome_log'],axis=1)


num_features=['ApplicantIncome_scaled',
    'CoapplicantIncome_norm',
    'LoanAmount_scaled',
    'Credit_History',
    'Property_Area_freq']

x=encoded_df[num_features]


# create polynoial feature transformer 
poly=PolynomialFeatures(degree=2,include_bias=False)
x_poly=poly.fit_transform(x)


# for column names for new features
feature_names=poly.get_feature_names_out(num_features)
df_poly=pd.DataFrame(x_poly,columns=feature_names)
df_poly.head()


df_poly=pd.concat([df.reset_index(drop=True),df_poly],axis=1)


df_poly


















